import jsonlines
from pathlib import Path
from ollama import Client
from generator import load_ollama_config

def load_jsonl(file_path):
    docs = []
    with jsonlines.open(file_path, 'r') as reader:
        for obj in reader:
            docs.append(obj)
    return docs

def save_jsonl(file_path, data):
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
    with jsonlines.open(file_path, mode='w') as writer:
        for item in data:
            writer.write(item)

def llm_generate(prompt: str, model: str = "granite4:3b") -> str:
    """
    Sends a prompt to the Ollama model and returns the response.

    Args:
        prompt: The prompt to send to the model.
        model: The name of the model to use.

    Returns:
        The model's response as a string.
    """
    try:
        config = load_ollama_config()
        client = Client(host=config["host"])
        """ 
            num_ctx, temperature, num_predict
            explaination in Final_Tutorials 4
        """
        response = client.generate(
            model=config["model"], 
            prompt=prompt, 
            stream=False, 
            options={
                "temperature": 0.0,
                "num_ctx": 5200  
            }
        )
        return response.get("response", "No response from model.")
    except Exception as e:
        return f"Error using Ollama Python client: {e}"