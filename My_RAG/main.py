import argparse
import os
import re

from chunker import chunk_documents
from database import ChromaDBManager
from generator import generate_answer
from retriever import create_dense_retriever,create_bm25_retriever,create_pyserini_retriever,SimpleHybridRetriever
from reranker import Reranker
from tqdm import tqdm
from utils import load_jsonl, save_jsonl
from generator import _domain_router_en,_domain_router_zh
# Reranker é…ç½®
USE_REMOTE_RERANKER = True  # True: æäº¤ç’°å¢ƒ(é ç¨‹API), False: æœ¬åœ°æ¸¬è©¦

# èªè¨€ç‰¹å®šé…ç½®
LANGUAGE_CONFIG = {
    "zh": {
        "use_rerank": True,
        "stage1_top_k": 20,
        "final_top_k": 3
    },
    "en": {
        "use_rerank": True,
        "stage1_top_k": 20,
        "final_top_k": 5
    }
}

def load_chunks_from_chroma(collection):
    """å¾ç¾æœ‰çš„ ChromaDB collection è®€å–æ‰€æœ‰è³‡æ–™ä¸¦é‚„åŸæˆ chunks åˆ—è¡¨"""
    print("æ­£åœ¨å¾ ChromaDB è®€å–å¿«å–è³‡æ–™...")
    
    # è®€å–æ‰€æœ‰è³‡æ–™ (åŒ…å« document å’Œ metadata)
    # limit=None ç¢ºä¿è®€å–å…¨éƒ¨ï¼Œinclude åƒæ•¸ç¢ºä¿æˆ‘å€‘æ‹¿åˆ°éœ€è¦çš„æ¬„ä½
    results = collection.get(include=["documents", "metadatas"])
    
    loaded_chunks = []
    total = len(results["ids"])
    
    for i in range(total):
        chunk = {
            "page_content": results["documents"][i],
            "metadata": results["metadatas"][i]
        }
        loaded_chunks.append(chunk)
        
    print(f"âœ… æˆåŠŸå¾ DB å¾©åŸ {len(loaded_chunks)} å€‹ Chunks (è·³é LLM ç”Ÿæˆ)")
    return loaded_chunks

def prepare_chroma_data(chunks):
    """æº–å‚™ ChromaDB éœ€è¦çš„æ•¸æ“šæ ¼å¼"""
    texts = []
    metadatas = []
    ids = []

    for idx, chunk in enumerate(chunks):
        texts.append(chunk["page_content"])

        # æå– metadata (åªä¿ç•™åŸºæœ¬é¡å‹)
        metadata = {}
        if "metadata" in chunk:
            for key, value in chunk["metadata"].items():
                if isinstance(value, (str, int, float, bool)):
                    metadata[key] = value
                else:
                    metadata[key] = str(value)
        metadatas.append(metadata)
        ids.append(str(idx))

    return texts, metadatas, ids


def main(
    query_path,
    docs_path,
    language,
    output_path,
    chroma_path="./my_vector_db",
    top_k=3
):
    # æ ¹æ“šèªè¨€ç²å–é…ç½®
    lang_config = LANGUAGE_CONFIG.get(language, {})
    use_rerank = lang_config.get("use_rerank", False)
    stage1_top_k = lang_config.get("stage1_top_k", 20)
    final_top_k = lang_config.get("final_top_k", 3)
    
    print(f"\n{'=' * 60}")
    print(f"é…ç½®ä¿¡æ¯:")
    print(f"  èªè¨€: {language}")
    print(f"  ä½¿ç”¨ Reranker: {use_rerank}")
    if use_rerank:
        print(f"  Reranker æ¨¡å¼: {'é ç¨‹API' if USE_REMOTE_RERANKER else 'æœ¬åœ°æ¨¡å‹'}")
        print(f"  Stage 1 å€™é¸æ•¸: {stage1_top_k}")
        print(f"  Stage 2 æœ€çµ‚æ•¸: {final_top_k}")
    else:
        print(f"  æª¢ç´¢æ•¸é‡: {final_top_k}")
    print(f"{'=' * 60}\n")
    
    # 1. Load Data
    print("Loading documents...")
    docs_for_chunking = load_jsonl(docs_path)
    queries = load_jsonl(query_path)
    print(f"Loaded {len(docs_for_chunking)} documents.")
    print(f"Loaded {len(queries)} queries.")

    # 2.è¼‰å…¥å…¬å¸åå–® (é€²è¡Œqueryæ­£è¦åŒ–æœç´¢)
    company_pattern = None
    try:
        if os.path.exists('./dragonball_dataset/company_names.txt'):
            with open('./dragonball_dataset/company_names.txt', 'r', encoding='utf-8') as f:
                # è®€å–ä¸¦å»é™¤ç©ºç™½
                company_list = [line.strip() for line in f if line.strip()]
            
            # é—œéµï¼šæŒ‰é•·åº¦ç”±å¤§åˆ°å°æ’åºï¼Œé¿å…ã€Œè¯å¤å¨›æ¨‚ã€åªåŒ¹é…åˆ°ã€Œè¯å¤ã€
            company_list.sort(key=len, reverse=True)
            
            if company_list:
                # å»ºç«‹ Regex Pattern: (åå¤å¨±ä¹æœ‰é™å…¬å¸|å†œä¸šå‘å±•æœ‰é™å…¬å¸|...)
                pattern_str = '|'.join(map(re.escape, company_list))
                company_pattern = re.compile(f"({pattern_str})")
                print(f"âœ… å·²è¼‰å…¥ {len(company_list)} é–“å…¬å¸åå–®ç”¨æ–¼éæ¿¾ã€‚")
        else:
            print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° company_names.txtï¼Œå°‡ä¸æœƒé€²è¡Œå…¬å¸éæ¿¾ã€‚")
    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥å…¬å¸åå–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    # 3. Chunk Documents
    chunks = []
    chroma_manager = None
    collection_name = f"docs_{language}"
    
    print(f"\n{'=' * 60}")
    print("Initializing ChromaDB & Checking Cache...")
    print(f"{'=' * 60}")

    try:
        # å…ˆé€£æ¥ ChromaDB
        chroma_manager = ChromaDBManager(
            persist_directory=chroma_path, collection_names=[collection_name]
        )
        collection = chroma_manager.get_collection(collection_name)
        existing_count = collection.count() if collection else 0

        # åˆ¤æ–·æ˜¯å¦éœ€è¦é‡æ–° Chunking
        if existing_count > 0:
            # Aè¨ˆç•«ï¼šDB è£¡æœ‰è³‡æ–™ -> ç›´æ¥æ‹¿å‡ºä¾†ç”¨
            print(f"âœ… æª¢æ¸¬åˆ°ç¾æœ‰ç´¢å¼• ({existing_count} items)ï¼Œè·³é LLM ç”Ÿæˆæ­¥é©Ÿã€‚")
            chunks = load_chunks_from_chroma(collection)
        else:
            # Bè¨ˆç•«ï¼šDB æ˜¯ç©ºçš„ -> åŸ·è¡Œæ˜‚è²´çš„ Chunking + LLM Context
            print("âš ï¸ æœªæª¢æ¸¬åˆ°ç´¢å¼•ï¼Œé–‹å§‹åŸ·è¡Œæ–‡æª”åˆ†å¡Šèˆ‡ LLM ä¸Šä¸‹æ–‡ç”Ÿæˆ (é€™æœƒèŠ±é»æ™‚é–“)...")
            chunks = chunk_documents(docs_for_chunking, language)
            print(f"Created {len(chunks)} chunks.")
            
            # æ¸…æ´— metadata (ä¿ç•™åŸæœ¬é‚è¼¯)
            print("Cleaning metadata entities...")
            for chunk in chunks:
                meta = chunk.get('metadata') if isinstance(chunk, dict) else chunk.metadata
                if meta:
                    if "hospital_patient_name" in meta and meta["hospital_patient_name"]:
                        full_name = meta["hospital_patient_name"]
                        clean_name = full_name.split('_')[0] 
                        meta["hospital_patient_name"] = clean_name
            
            # å­˜å…¥ ChromaDB
            print(f"Building ChromaDB index for {len(chunks)} chunks...")
            texts, metadatas, ids = prepare_chroma_data(chunks)
            success = chroma_manager.save_chunks_to_chroma(
                collection_name=collection_name,
                texts=texts,
                metadatas=metadatas,
                ids=ids,
                batch_size=500,
            )
            if not success:
                print("âš ï¸ ChromaDB indexing failed.")

    except Exception as e:
        print(f"âš ï¸ ChromaDB Error: {e}")
        # å¦‚æœ DB æ›äº†ï¼Œè¿«ä¸å¾—å·²åªå¥½ç¾å ´é‡ç®— (Fallback)
        if not chunks:
            print("Fallback: Re-calculating chunks in memory...")
            chunks = chunk_documents(docs_for_chunking, language)

    # 4. Create Retriever
    ## å¼·åˆ¶ä¿®æ”¹æˆå…©å€‹retrieverçµæœéƒ½è¦ç”¨åˆ°ä¸¦ä½œhybrid
    print(f"\n{'=' * 60}")
    print("Creating retriever...")
    print(f"{'=' * 60}")

    dense_retriever = create_dense_retriever(
        chunks=chunks,
        language=language,
        chroma_manager=chroma_manager,
    )
    
    pyserini_retriever = create_pyserini_retriever(
        chunks=chunks,
        language=language
    )
    # è¨­å®šæ¬Šé‡
    if language == "zh":
        # ä¸­æ–‡ç’°å¢ƒï¼šé€šå¸¸ BM25 å°å°ˆæœ‰åè©æ›´æº–ï¼Œæ¬Šé‡çµ¦é«˜ä¸€é»
        weights = {"dense": 0.4, "sparse": 0.6}
    else:
        # è‹±æ–‡ç’°å¢ƒï¼šä¸€èˆ¬é è¨­ 0.5/0.5 æˆ–è¦–æƒ…æ³èª¿æ•´
        weights = {"dense": 0.5, "sparse": 0.5}

    print(f"Initializing Hybrid Retriever with weights: {weights}")
    hybrid_retriever = SimpleHybridRetriever(
        dense_retriever=dense_retriever,
        sparse_retriever=pyserini_retriever,
        weights=weights,
        language=language
    )    

    # 5. Initialize Reranker (if needed)
    reranker = None
    if use_rerank:
        print(f"\n{'=' * 60}")
        print("Initializing Reranker...")
        print(f"{'=' * 60}")
        
        try:
            reranker = Reranker(
                mode="remote" if USE_REMOTE_RERANKER else "local"
            )
            print(f"âœ… Reranker initialized successfully")
        except Exception as e:
            print(f"âš ï¸ Reranker initialization failed: {e}")
            print("âš ï¸ å°‡ä½¿ç”¨å–®éšæ®µæª¢ç´¢")
            use_rerank = False

    # 6. Process Queries
    print(f"\n{'=' * 60}")
    print("Processing queries...")
    print(f"{'=' * 60}")

    for query in tqdm(queries, desc="Processing Queries"):
        query_text = query["query"]["content"]
        # æ”¹ç”¨ findall æŠ“å–æ‰€æœ‰å…¬å¸åç¨±
        target_companies = []
        if company_pattern:
            # findall æœƒå›å‚³ä¸€å€‹ listï¼ŒåŒ…å«æ‰€æœ‰åŒ¹é…çš„å­—ä¸²
            found = company_pattern.findall(query_text)
            
            # å»é™¤é‡è¤‡ (set) ä¸¦éæ¿¾é›œè¨Š
            target_companies = list(set(found))
            
            if target_companies:
                print(f"åµæ¸¬åˆ°å…¬å¸: {target_companies}")  # é™¤éŒ¯: æ‡‰è©²è¦çœ‹åˆ° ['CleanCo', 'Retail Emporium']

        # å»ºç«‹ ChromaDB éœ€è¦çš„ filter
        where_filter = None
        
        if target_companies:
            # å®šç¾©ä½ è¦æœå°‹çš„æ‰€æœ‰ Metadata æ¬„ä½åç¨±
            # è«‹ç¢ºä¿é€™è£¡çš„ key èˆ‡ä½  ingest å…¥åº«æ™‚çš„ key ä¸€æ¨¡ä¸€æ¨£
            search_keys = ["company_name", "court_name", "hospital_patient_name"]
            
            # å»ºç«‹æ‰€æœ‰å¯èƒ½çš„çµ„åˆæ¢ä»¶
            # é‚è¼¯ï¼š(å…¬å¸åæ˜¯A OR æ³•é™¢åæ˜¯A OR é†«é™¢åæ˜¯A) OR (å…¬å¸åæ˜¯B OR ...)
            or_conditions = []
            for entity in target_companies:
                for key in search_keys:
                    or_conditions.append({key: entity})
            
            # ç”Ÿæˆ Filter
            if len(or_conditions) == 1:
                # æ¥µå°‘è¦‹æƒ…æ³ï¼šåªæœä¸€å€‹åç¨±ä¸”åªæœä¸€å€‹æ¬„ä½
                where_filter = or_conditions[0]
            else:
                # çµ•å¤§å¤šæ•¸æƒ…æ³éƒ½æœƒèµ°é€™è£¡ï¼Œå› ç‚ºæ¯å€‹åç¨±éƒ½è¦æœ 3 å€‹æ¬„ä½
                where_filter = {"$or": or_conditions}
        
        # Stage 1: æª¢ç´¢å€™é¸æ–‡æª”
        if use_rerank:
            # ä½¿ç”¨ reranker: å…ˆæª¢ç´¢æ›´å¤šå€™é¸
            retrieve_k = stage1_top_k
        else:
            # ä¸ä½¿ç”¨ reranker: ç›´æ¥æª¢ç´¢æœ€çµ‚æ•¸é‡
            retrieve_k = final_top_k
            
        retrieved_chunks = hybrid_retriever.retrieve(
                query_text, 
                top_k=retrieve_k, 
                where_filter=where_filter
            )
        #retrieved_chunks = dense_retriever.retrieve(query_text,retrieve_k,where_filter)
        # =================================================
        # ğŸŸ¢ æ–°å¢ï¼šå»é‡é‚è¼¯ (Deduplication)
        # =================================================
        seen_ids = set()
        unique_chunks = []
        for chunk in retrieved_chunks:
            # å„ªå…ˆå˜—è©¦æŠ“å– metadata è£¡çš„ idï¼Œå¦‚æœæ²’æœ‰å‰‡é€€è€Œæ±‚å…¶æ¬¡ç”¨å…§å®¹æœ¬èº«ç•¶ key
            # å‡è¨­ chunk æ˜¯ dict æˆ– objectï¼Œé€™è£¡åšå€‹ç›¸å®¹æ€§è™•ç†
            if isinstance(chunk, dict):
                c_id = chunk.get("metadata", {}).get("id") or chunk.get("page_content")
            else: # å‡è¨­æ˜¯ Document ç‰©ä»¶
                c_id = chunk.metadata.get("id") or chunk.page_content
            
            if c_id not in seen_ids:
                seen_ids.add(c_id)
                unique_chunks.append(chunk)
        
        # å°‡å»é‡å¾Œçš„çµæœæŒ‡æ´¾å›å»
        retrieved_chunks = unique_chunks
        # =================================================
        # Stage 2: Rerankingï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if use_rerank and reranker is not None and retrieved_chunks:
            print("åŸ·è¡Œreranker!")
            retrieved_chunks = reranker.rerank(
                query=query_text,
                chunks=retrieved_chunks,
                top_k=final_top_k,
                return_scores=True,
            )

        # ç”Ÿæˆç­”æ¡ˆ
        if language == "zh":
            query_domain = _domain_router_zh(query_text,retrieved_chunks)
            answer = generate_answer(query_text, retrieved_chunks, language, query_domain)
        else:
            query_domain = _domain_router_en(query_text,retrieved_chunks)
            answer = generate_answer(query_text, retrieved_chunks,language,query_domain)

        query["prediction"]["content"] = answer
        
        # å„²å­˜ Referencesï¼ˆæ ¹æ“šèªè¨€åˆ†é›¢ç­–ç•¥ï¼‰
        if language == "zh":
            # ä¸­æ–‡ï¼šä¿å­˜æ‰€æœ‰ chunks
            query["prediction"]["references"] = [
                chunk["page_content"] for chunk in retrieved_chunks
            ]
        else: 
            query["prediction"]["references"] = [
                chunk["page_content"] for chunk in retrieved_chunks
            ]


    # 7. Save Results
    save_jsonl(output_path, queries)
    print(f"\n{'=' * 60}")
    print(f"âœ… Predictions saved at '{output_path}'")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="RAG System with Optional Hybrid Retrieval"
    )

    # åŸæœ‰çš„åŸºæœ¬åƒæ•¸
    parser.add_argument("--query_path", required=True, help="Path to the query file")
    parser.add_argument("--docs_path", required=True, help="Path to the documents file")
    parser.add_argument("--language", required=True, help="Language (zh or en)")
    parser.add_argument("--output", required=True, help="Path to the output file")

    parser.add_argument(
        "--chroma_path", default="./my_vector_db", help="ChromaDB storage path"
    )
    parser.add_argument(
        "--top_k", type=int, default=3, help="Number of chunks to retrieve"
    )
    args = parser.parse_args()

    main(
        query_path=args.query_path,
        docs_path=args.docs_path,
        language=args.language,
        output_path=args.output,
        chroma_path=args.chroma_path,
        top_k=args.top_k
    )
