from typing import Dict, List, Optional

import jieba
import numpy as np
from rank_bm25 import BM25Okapi
import os
import json
import shutil
import subprocess
from pyserini.search.lucene import LuceneSearcher

class SimpleHybridRetriever:
    def __init__(self, dense_retriever, sparse_retriever, weights={"dense": 0.5, "sparse": 0.5}):
        self.dense_retriever = dense_retriever
        self.sparse_retriever = sparse_retriever
        self.weights = weights

    def _normalize_scores(self, results):
        """Min-Max Normalization"""
        if not results:
            return []
        scores = [r["score"] for r in results]
        min_score = min(scores)
        max_score = max(scores)
        if max_score == min_score:
            return results
        for r in results:
            r["normalized_score"] = (r["score"] - min_score) / (max_score - min_score)
        return results

    def _matches_filter(self, metadata, where_filter):
        """
        æ‰‹å‹•æª¢æŸ¥ metadata æ˜¯å¦ç¬¦åˆ ChromaDB é¢¨æ ¼çš„ filter
        æ”¯æ´ç°¡å–®çš„ {"key": "value"} å’Œ {"$or": [...]}
        """
        if not where_filter:
            return True
        if not metadata:
            return False

        # è™•ç† $or é‚è¼¯ (æ‚¨çš„ä»£ç¢¼ä¸­æœ‰ç”¨åˆ°)
        if "$or" in where_filter:
            conditions = where_filter["$or"]
            for cond in conditions:
                # å‡è¨­ cond æ˜¯å–®ä¸€éµå€¼å° {'company_name': 'XXX'}
                k, v = list(cond.items())[0]
                if metadata.get(k) == v:
                    return True
            return False
            
        # è™•ç†å–®ä¸€æ¢ä»¶ (ä¸€èˆ¬æƒ…æ³)
        for key, value in where_filter.items():
            if metadata.get(key) != value:
                return False
        return True

    def retrieve(self, query, top_k=5, where_filter=None):
        # 1. æ“´å¤§å€™é¸ç¯„åœ (å› ç‚ºéæ¿¾å¾Œæ•¸é‡æœƒè®Šå°‘)
        candidate_k = top_k * 3 
        
        # 2. åŸ·è¡Œæª¢ç´¢
        # Dense è‡ªå¸¶éæ¿¾ï¼Œæ‰€ä»¥çµæœå·²ç¶“æ˜¯ä¹¾æ·¨çš„
        dense_results = self.dense_retriever.retrieve(query, top_k=candidate_k, where_filter=where_filter)
        
        # Sparse (Pyserini) ä¸å¸¶éæ¿¾ï¼Œæœƒå›å‚³é«’è³‡æ–™
        sparse_results = self.sparse_retriever.retrieve(query, top_k=candidate_k)

        # 3. å° Sparse çµæœé€²è¡Œã€Œå¾Œè™•ç†éæ¿¾ã€ (é—œéµä¿®æ­£ï¼)
        if where_filter:
            filtered_sparse = []
            for doc in sparse_results:
                # é€™è£¡å‡è¨­ doc è£¡æœ‰ 'metadata' æ¬„ä½ï¼Œä¸”å…§å®¹çµæ§‹æ­£ç¢º
                if self._matches_filter(doc.get("metadata"), where_filter):
                    filtered_sparse.append(doc)
            sparse_results = filtered_sparse

        # 4. æ­£è¦åŒ–
        dense_results = self._normalize_scores(dense_results)
        sparse_results = self._normalize_scores(sparse_results)

        # 5. åŠ æ¬Šèåˆ
        fused_scores = {}
        doc_map = {}

        for doc in dense_results:
            cid = doc.get("chunk_id") or doc.get("id")
            doc_map[cid] = doc
            fused_scores[cid] = fused_scores.get(cid, 0.0) + (doc.get("normalized_score", 0.0) * self.weights["dense"])

        for doc in sparse_results:
            cid = doc.get("chunk_id") or doc.get("id")
            if cid not in doc_map:
                doc_map[cid] = doc
            fused_scores[cid] = fused_scores.get(cid, 0.0) + (doc.get("normalized_score", 0.0) * self.weights["sparse"])

        # 6. æ’åºèˆ‡å– Top K
        sorted_ids = sorted(fused_scores.items(), key=lambda item: item[1], reverse=True)[:top_k]
        
        final_results = []
        for cid, score in sorted_ids:
            chunk = doc_map[cid].copy()
            chunk["score"] = score
            if "normalized_score" in chunk:
                del chunk["normalized_score"]
            final_results.append(chunk)

        return final_results
    
class PyseriniRetriever:
    """
    åŸºæ–¼ Pyserini (Lucene) çš„ BM25 æª¢ç´¢å™¨ã€‚
    ç‰¹é»ï¼š
    1. ä¸ä½”ç”¨å¤§é‡ RAM (ç´¢å¼•å­˜æ–¼ Disk)
    2. æª¢ç´¢é€Ÿåº¦æ¥µå¿«
    3. å…§å»ºå¤šèªè¨€ Analyzer (ä¸éœ€è¦æ‰‹å‹• jieba)
    """

    def __init__(self, chunks, language, index_path="./pyserini_index"):
        self.chunks = chunks
        self.language = language
        self.index_path = f"{index_path}_{language}"  # å€åˆ†ä¸­è‹±æ–‡ç´¢å¼•è·¯å¾‘
        
        # 1. æº–å‚™ç´¢å¼•è³‡æ–™
        # å¦‚æœç´¢å¼•å·²ç¶“å­˜åœ¨ï¼Œæˆ‘å€‘å¯ä»¥é¸æ“‡è·³éé‡å»ºï¼Œæˆ–è€…å¼·åˆ¶é‡å»º (é€™è£¡è¨­ç‚ºå¼·åˆ¶é‡å»ºä»¥ç¢ºä¿è³‡æ–™æœ€æ–°)
        if os.path.exists(self.index_path):
            shutil.rmtree(self.index_path)
            
        self._build_index()

        # 2. è¼‰å…¥ Searcher
        self.searcher = LuceneSearcher(self.index_path)
        
        # 3. è¨­å®šèªè¨€åƒæ•¸ (é€™å¾ˆé‡è¦ï¼Œæ±ºå®šäº†å®ƒå¦‚ä½•åˆ†è©)
        if language == "zh":
            self.searcher.set_language("zh")  # ä½¿ç”¨ Lucene å…§å»ºçš„ä¸­æ–‡åˆ†è©
        else:
            self.searcher.set_language("en")  # ä½¿ç”¨æ¨™æº–è‹±æ–‡åˆ†è©

    def _build_index(self):
        """å°‡ chunks è½‰æ›ç‚º JSONL ä¸¦èª¿ç”¨ Pyserini å»ºç«‹ç´¢å¼•"""
        print(f"æ­£åœ¨ç‚º {len(self.chunks)} ç­†è³‡æ–™å»ºç«‹ Pyserini ç´¢å¼•...")
        
        # å»ºç«‹æš«å­˜è³‡æ–™å¤¾æ”¾ input.jsonl
        input_dir = "./temp_pyserini_input"
        if os.path.exists(input_dir):
            shutil.rmtree(input_dir)
        os.makedirs(input_dir)

        jsonl_path = os.path.join(input_dir, "docs.jsonl")
        
        # å°‡ chunks å¯«å…¥ç¬¦åˆ Pyserini æ ¼å¼çš„ JSONL (id, contents)
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for idx, chunk in enumerate(self.chunks):
                doc = {
                    "id": str(idx),  # ä½¿ç”¨ list index ä½œç‚º idï¼Œæ–¹ä¾¿æ‰¾å›
                    "contents": chunk["page_content"]
                }
                f.write(json.dumps(doc, ensure_ascii=False) + '\n')

        # æ§‹å»ºç´¢å¼•æŒ‡ä»¤
        # Pyserini é€šå¸¸å»ºè­°ä½¿ç”¨ subprocess èª¿ç”¨å‘½ä»¤è¡Œå·¥å…·é€²è¡Œç´¢å¼•
        cmd = [
            "python", "-m", "pyserini.index.lucene",
            "--collection", "JsonCollection",
            "--input", input_dir,
            "--index", self.index_path,
            "--generator", "DefaultLuceneDocumentGenerator",
            "--threads", "1",
            "--storePositions", "--storeDocvectors", "--storeRaw"
        ]

        # å¦‚æœæ˜¯ä¸­æ–‡ï¼Œéœ€è¦åœ¨ç´¢å¼•éšæ®µæŒ‡å®šèªè¨€
        if self.language == "zh":
            cmd.extend(["--language", "zh"])

        # åŸ·è¡Œç´¢å¼•å‘½ä»¤ (éœé»˜æ¨¡å¼ï¼Œè‹¥è¦çœ‹ log å¯ç§»é™¤ stdout=subprocess.DEVNULL)
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        
        # æ¸…ç†æš«å­˜æª”
        shutil.rmtree(input_dir)
        print("ç´¢å¼•å»ºç«‹å®Œæˆã€‚")

    def retrieve(self, query, top_k=3):
        """æª¢ç´¢ä¸¦è¿”å›å¸¶åˆ†æ•¸çš„çµæœ"""
        
        # åŸ·è¡Œæœå°‹
        # Pyserini æœƒè‡ªå‹•æ ¹æ“š set_language è™•ç† query çš„åˆ†è©
        hits = self.searcher.search(query, k=top_k)

        results = []
        for hit in hits:
            # hit.docid æ˜¯æˆ‘å€‘åœ¨ _build_index æ™‚å­˜å…¥çš„ str(idx)
            chunk_idx = int(hit.docid)
            
            # è¤‡è£½åŸå§‹ chunk ä¸¦åŠ å…¥åˆ†æ•¸
            chunk = self.chunks[chunk_idx].copy()
            chunk["score"] = float(hit.score)
            chunk["chunk_id"] = str(chunk_idx)
            
            # (å¯é¸) Pyserini å…¶å¯¦ä¹Ÿå­˜äº† raw contentï¼Œå¯ä»¥ç”¨ hit.raw å–å¾—
            # ä½†æ—¢ç„¶æˆ‘å€‘æœ‰ self.chunksï¼Œç›´æ¥æŸ¥è¡¨æœ€å¿«
            
            results.append(chunk)

        return results

class BM25Retriever:
    """BM25 é—œéµå­—æª¢ç´¢å™¨"""

    def __init__(self, chunks, language="en"):
        self.chunks = chunks
        self.language = language
        self.corpus = [chunk["page_content"] for chunk in chunks]

        if language == "zh":
            self.tokenized_corpus = [list(jieba.cut(doc)) for doc in self.corpus]
        else:
            self.tokenized_corpus = [doc.split(" ") for doc in self.corpus]

        self.bm25 = BM25Okapi(self.tokenized_corpus)

    def retrieve(self, query, top_k=3):
        """æª¢ç´¢ä¸¦è¿”å›å¸¶åˆ†æ•¸çš„çµæœ"""
        if self.language == "zh":
            tokenized_query = list(jieba.cut(query))
        else:
            tokenized_query = query.split(" ")

        # ç²å–æ‰€æœ‰æ–‡æª”çš„åˆ†æ•¸
        scores = self.bm25.get_scores(tokenized_query)

        # ç²å– top_k çš„ç´¢å¼•
        top_indices = np.argsort(scores)[::-1][:top_k]

        # æ§‹å»ºçµæœåˆ—è¡¨
        results = []
        for idx in top_indices:
            chunk = self.chunks[idx].copy()
            chunk["score"] = float(scores[idx])
            chunk["chunk_id"] = str(idx)
            results.append(chunk)

        return results


# class HybridRetriever:
#     """æ··åˆæª¢ç´¢å™¨ï¼šBM25 + Dense Embedding (ChromaDB)"""

#     def __init__(self, chunks, language, chroma_manager=None):
#         self.chunks = chunks
#         self.language = language

#         # BM25 æª¢ç´¢å™¨
#         self.bm25_retriever = BM25Retriever(chunks, language)

#         # ChromaDB æª¢ç´¢å™¨
#         self.chroma_manager = chroma_manager
#         self.collection_name = f"docs_{language}" if chroma_manager else None

#         # å¯èª¿åƒæ•¸
#         self.alpha = 0.7  # BM25 æ¬Šé‡ (0-1)ï¼Œvector æ¬Šé‡ç‚º (1-alpha)
#         self.rrf_k = 60  # RRF å¹³æ»‘åƒæ•¸

#     def retrieve(self, query, top_k=3, method="rrf",where_filter=None):
#         """
#         æ··åˆæª¢ç´¢ä¸»å‡½æ•¸

#         Args:
#             query: æŸ¥è©¢æ–‡æœ¬
#             top_k: æœ€çµ‚è¿”å›æ•¸é‡
#             method: åˆä½µæ–¹æ³• ("rrf" æˆ– "weighted")

#         Returns:
#             List of chunks with scores
#         """
#         # å¦‚æœæ²’æœ‰ ChromaDBï¼Œé€€å›åˆ°ç´” BM25
#         if self.chroma_manager is None:
#             print("ä½¿ç”¨ç´”BM25")
#             return self.bm25_retriever.retrieve(query, top_k)

#         # 1. BM25 æª¢ç´¢ (å– 2*top_k å¢åŠ è¦†è“‹ç‡)
#         bm25_results = self.bm25_retriever.retrieve(query, top_k=top_k * 2)

#         # 2. Vector æª¢ç´¢ (å– 2*top_k)
#         try:
#             chroma_results = self.chroma_manager.query_chunks(
#                 collection_name=self.collection_name, query_text=query, top_k=top_k * 2,where_filter=where_filter
#             )

#             # å°‡ ChromaDB çµæœè½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
#             vector_results = self._parse_chroma_results(chroma_results)

#             if not vector_results:
#                 print("Warning: Vector search returned no results, using BM25 only")
#                 return bm25_results[:top_k]

#         except Exception as e:
#             print(f"Vector search failed: {e}, falling back to BM25 only")
#             return bm25_results[:top_k]

#         # 3. åˆä½µçµæœ
#         if method == "rrf":
#             merged_results = self._reciprocal_rank_fusion(
#                 bm25_results, vector_results, top_k
#             )
#         else:  # weighted
#             merged_results = self._weighted_merge(bm25_results, vector_results, top_k)

#         return merged_results

#     def _parse_chroma_results(self, chroma_results: Optional[Dict]) -> List[Dict]:
#         """
#         å°‡ ChromaDB çš„æŸ¥è©¢çµæœè½‰æ›ç‚ºçµ±ä¸€æ ¼å¼

#         ChromaDB è¿”å›æ ¼å¼:
#         {
#             'ids': [['id1', 'id2', ...]],
#             'documents': [['text1', 'text2', ...]],
#             'metadatas': [[{...}, {...}, ...]],
#             'distances': [[0.5, 0.7, ...]]  # è·é›¢è¶Šå°è¶Šç›¸ä¼¼
#         }
#         """
#         if not chroma_results:
#             return []

#         # ChromaDB è¿”å›çš„æ˜¯åµŒå¥—åˆ—è¡¨
#         ids = chroma_results.get("ids", [[]])[0]
#         documents = chroma_results.get("documents", [[]])[0]
#         metadatas = chroma_results.get("metadatas", [[]])[0]
#         distances = chroma_results.get("distances", [[]])[0]

#         results = []
#         for i, doc_id in enumerate(ids):
#             chunk_id = str(doc_id)

#             # åªå°ç¬¬ä¸€ç­†ç¢ºèª ID è½‰æ›
#             if i == 0:
#                 print(
#                     f"\nğŸ” [ID Mapping Check] åŸå§‹={doc_id!r} ({type(doc_id).__name__}) â†’ è½‰æ›å¾Œ={chunk_id!r} ({type(chunk_id).__name__})"
#                 )

#             # å°‡è·é›¢è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸ (è·é›¢è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜)
#             # ä½¿ç”¨å…¬å¼: similarity = 1 / (1 + distance)
#             distance = distances[i] if i < len(distances) else 1.0
#             similarity_score = 1.0 / (1.0 + distance)

#             result = {
#                 "chunk_id": str(doc_id),
#                 "page_content": documents[i] if i < len(documents) else "",
#                 "metadata": metadatas[i] if i < len(metadatas) else {},
#                 "score": similarity_score,
#                 "distance": distance,
#             }
#             results.append(result)

#         return results

#     def _weighted_merge(
#         self, bm25_results: List[Dict], vector_results: List[Dict], top_k: int
#     ) -> List[Dict]:
#         """
#         æ–¹æ³• A: åŠ æ¬Šå¹³å‡åˆä½µ
#         é©åˆ: ç•¶ä½ æƒ³æ˜ç¢ºæ§åˆ¶ BM25 å’Œ Vector çš„å½±éŸ¿åŠ›
#         """
#         scores = {}
#         chunk_data = {}

#         # æ­£è¦åŒ– BM25 åˆ†æ•¸åˆ° 0-1
#         max_bm25 = max([r["score"] for r in bm25_results], default=1e-6)
#         if max_bm25 == 0:
#             max_bm25 = 1e-6

#         # è™•ç† BM25 çµæœ
#         for result in bm25_results:
#             chunk_id = result["chunk_id"]  # å·²ç¶“æ˜¯å­—ä¸²
#             normalized_score = result["score"] / max_bm25
#             scores[chunk_id] = self.alpha * normalized_score
#             chunk_data[chunk_id] = result

#         # è™•ç† Vector çµæœ
#         for result in vector_results:
#             chunk_id = result["chunk_id"]
#             vector_score = result["score"]  # å·²ç¶“æ˜¯ç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)

#             if chunk_id in scores:
#                 scores[chunk_id] += (1 - self.alpha) * vector_score
#             else:
#                 scores[chunk_id] = (1 - self.alpha) * vector_score
#                 chunk_data[chunk_id] = result

#         # æ’åºä¸¦æ§‹å»ºæœ€çµ‚çµæœ
#         sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)

#         merged_results = []
#         for chunk_id in sorted_ids[:top_k]:
#             chunk = chunk_data[chunk_id].copy()
#             chunk["score"] = scores[chunk_id]
#             chunk["chunk_id"] = chunk_id
#             merged_results.append(chunk)

#         return merged_results

#     def _reciprocal_rank_fusion(
#         self, bm25_results: List[Dict], vector_results: List[Dict], top_k: int
#     ) -> List[Dict]:
#         """
#         æ–¹æ³• B: Reciprocal Rank Fusion (RRF)
#         é©åˆ: ä¸ç¢ºå®šå“ªå€‹æª¢ç´¢å™¨æ›´å¥½æ™‚ï¼Œè®“æ•¸æ“šèªªè©±

#         å…¬å¼: score(d) = Î£ 1 / (k + rank_i(d))
#         """
#         scores = {}
#         chunk_data = {}

#         # è™•ç† BM25 æ’å
#         for rank, result in enumerate(bm25_results):
#             chunk_id = result["chunk_id"]
#             scores[chunk_id] = 1.0 / (self.rrf_k + rank + 1)
#             chunk_data[chunk_id] = result

#         # è™•ç† Vector æ’å (ç´¯åŠ )
#         for rank, result in enumerate(vector_results):
#             chunk_id = result["chunk_id"]
#             rrf_score = 1.0 / (self.rrf_k + rank + 1)

#             if chunk_id in scores:
#                 scores[chunk_id] += rrf_score
#                 print("å®å’š ç´¯åŠ é‡è¤‡è¨ˆç®—ï¼")
#             else:
#                 scores[chunk_id] = rrf_score
#                 chunk_data[chunk_id] = result

#         # æ’åºä¸¦æ§‹å»ºæœ€çµ‚çµæœ
#         sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)

#         merged_results = []
#         for chunk_id in sorted_ids[:top_k]:
#             chunk = chunk_data[chunk_id].copy()
#             chunk["score"] = scores[chunk_id]
#             chunk["chunk_id"] = chunk_id
#             merged_results.append(chunk)

#         return merged_results

#     def set_params(self, alpha=None, rrf_k=None):
#         """å‹•æ…‹èª¿æ•´åƒæ•¸"""
#         if alpha is not None:
#             self.alpha = alpha
#         if rrf_k is not None:
#             self.rrf_k = rrf_k

class DenseRetriever:
    def __init__(self, chunks, language, chroma_manager=None):
        self.chunks = chunks
        self.language = language

        # ChromaDB æª¢ç´¢å™¨
        self.chroma_manager = chroma_manager
        self.collection_name = f"docs_{language}" if chroma_manager else None


    def retrieve(self, query, top_k=3, where_filter=None):
        """
        æ··åˆæª¢ç´¢ä¸»å‡½æ•¸

        Args:
            query: æŸ¥è©¢æ–‡æœ¬
            top_k: æœ€çµ‚è¿”å›æ•¸é‡
            method: åˆä½µæ–¹æ³• ("rrf" æˆ– "weighted")

        Returns:
            List of chunks with scores
        """
        if self.chroma_manager is None:
            print("æ²’æœ‰åˆå§‹åŒ–chroma")

        # Vector æª¢ç´¢
        try:
            chroma_results = self.chroma_manager.query_chunks(
                collection_name=self.collection_name, query_text=query, top_k=top_k,where_filter=where_filter
            )

            # å°‡ ChromaDB çµæœè½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
            vector_results = self._parse_chroma_results(chroma_results)

        except Exception as e:
            print(f"Vector search failed: {e}, falling back to BM25 only")

        return vector_results

    def _parse_chroma_results(self, chroma_results: Optional[Dict]) -> List[Dict]:
        """
        å°‡ ChromaDB çš„æŸ¥è©¢çµæœè½‰æ›ç‚ºçµ±ä¸€æ ¼å¼

        ChromaDB è¿”å›æ ¼å¼:
        {
            'ids': [['id1', 'id2', ...]],
            'documents': [['text1', 'text2', ...]],
            'metadatas': [[{...}, {...}, ...]],
            'distances': [[0.5, 0.7, ...]]  # è·é›¢è¶Šå°è¶Šç›¸ä¼¼
        }
        """
        if not chroma_results:
            return []

        # ChromaDB è¿”å›çš„æ˜¯åµŒå¥—åˆ—è¡¨
        ids = chroma_results.get("ids", [[]])[0]
        documents = chroma_results.get("documents", [[]])[0]
        metadatas = chroma_results.get("metadatas", [[]])[0]
        distances = chroma_results.get("distances", [[]])[0]

        results = []
        for i, doc_id in enumerate(ids):
            # å°‡è·é›¢è½‰æ›ç‚ºç›¸ä¼¼åº¦åˆ†æ•¸ (è·é›¢è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜)
            # ä½¿ç”¨å…¬å¼: similarity = 1 - distance
            distance = distances[i]
            similarity_score = 1.0 - distance

            result = {
                "chunk_id": int(doc_id) if doc_id.isdigit() else doc_id,
                "page_content": documents[i] if i < len(documents) else "",
                "metadata": metadatas[i] if i < len(metadatas) else {},
                "score": similarity_score,
                "distance": distance,
            }
            results.append(result)

        return results

def create_dense_retriever(chunks, language, chroma_manager=None):
    print("Creating Dense Retriever")
    return DenseRetriever(chunks, language, chroma_manager)

def create_bm25_retriever(chunks, language):
    print("Creating BM25 Retriever...")
    return BM25Retriever(chunks, language)

def create_pyserini_retriever(chunks, language):
    print("Creating pyserini Retriever...")
    return PyseriniRetriever(chunks, language)
# def create_retriever(chunks, language, chroma_manager=None, use_hybrid=True):
#     """
#     å‰µå»ºæª¢ç´¢å™¨

#     Args:
#         chunks: æ–‡æª”å¡Šåˆ—è¡¨
#         language: èªè¨€ ("zh" æˆ– "en")
#         chroma_manager: ChromaDBManager å¯¦ä¾‹ (å¯é¸)
#         use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæª¢ç´¢

#     Returns:
#         BM25Retriever æˆ– HybridRetriever
#     """
#     if use_hybrid and chroma_manager is not None:
#         print("Creating Hybrid Retriever (BM25 + Vector)...")
#         return HybridRetriever(chunks, language, chroma_manager)
#     else:
#         print("Creating BM25 Retriever...")
#         return BM25Retriever(chunks, language)
